# ğŸš€ AI-Powered Visual Web Scraper Builder ğŸ”§

This Full Stack SaaS application allows users to visually build, manage, and schedule web scrapers using a workflow builder powered by AI. ğŸ¤– Users can create, modify, and delete workflows with an intuitive drag-and-drop interface. The integration of AI simplifies the web scraping process, making it accessible to both technical and non-technical users. ğŸŒ

## ğŸ”¥ Features

### ğŸŒŸ Key Features

-   **ğŸ” Visual Workflow Builder**: Drag-and-drop interface to design scraping workflows effortlessly. ğŸ”§
-   **ğŸ¤– AI Assistance**: AI-powered suggestions for selectors, workflow optimization, and error handling. ğŸŒ
-   **ğŸ”‘ Credential Management**: Securely manage login credentials for scraping protected websites. ğŸ”’
-   **â³ Scheduling System**: Set up automatic scraping schedules for periodic data extraction. â°
-   **ğŸ›ï¸ Workflow Management**: Create, modify, delete, and duplicate workflows with ease. ğŸ”„
-   **ğŸ“„ Data Export**: Export scraped data in various formats (e.g., CSV, JSON). ğŸ“Š

### âš¡ Built with Next.js

-   **ğŸŒ Server-Side Rendering (SSR)** for optimized SEO and performance.
-   **ğŸŒ API Routes** to handle backend logic.
-   **ğŸ“ Dynamic Routing** for user-specific workflows.
-   **ğŸ” Built-in Authentication** using Clerk Authentication for secure user sessions.

---

### âš™ï¸ Prerequisites

-   **ğŸ’» Node.js** (v16 or later)
-   **ğŸ“Š PostgreSQL** database
-   **ğŸ”§ API Key for OpenAI

### ğŸ” Steps

1. **ğŸ”„ Clone the repository**

    ```bash
    git clone https://github.com/BernieTv/Workflows-App.git
    cd Workflows-App
    ```

2. **ğŸ“¦ Install dependencies**

    ```bash
    npm install
    ```

3. **ğŸ” Set up environment variables**
   Create a `.env` file in the root directory and add the following:

    ```env
    NEXT_PUBLIC_OPENAI_API_KEY=your_openai_api_key
    DATABASE_URL=postgresql://username:password@localhost:5432/yourdb
    NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=your_clerk_secret
    CLERK_SECRET_KEY=sk_
    ```

4. **ğŸ”§ Run database migrations**

    ```bash
    npx prisma migrate dev
    ```

5. **ğŸš€ Start the development server**
    ```bash
    npm run dev
    ```
    The app will be available at `http://localhost:3000`. ğŸ 

---

## ğŸ”§ Usage

### 1. **ğŸ” Sign Up/Log In**

-   Use Clerk Authentication to sign up or log in to your account. ğŸ”‘

### 2. **ğŸ”§ Create a Workflow**

-   Drag and drop nodes to define scraping tasks. ğŸŒ
-   Use AI suggestions for selector optimization. ğŸ¤–

### 3. **ğŸ”‘ Set Credentials**

-   Securely store website login credentials if required. ğŸ”’

### 4. **â³ Schedule Scraping**

-   Use the scheduling feature to automate scraping tasks. ğŸ•°ï¸

### 5. **ğŸ“„ Export Data**

-   Download scraped data in the desired format. ğŸ“Š

---

## ğŸ’ª Development

### âš™ï¸ Scripts

-   **ğŸš€ Start development server**: `npm run dev`
-   **ğŸŒ Build for production**: `npm run build`
-   **ğŸ  Run production server**: `npm start`

### ğŸ”§ Linting and Formatting

-   **ğŸ”¢ Lint code**: `npm run lint`
-   **ğŸ”„ Format code**: `npm run format`

